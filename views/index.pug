extends layout

block title
  title Phrase Indexed Question Answering

block description
  meta(name='description', content='We formalize a new modular variant of current question answering tasks by enforcing complete independence of the document encoder from the question encoder. This formulation addresses a key challenge in machine comprehension by requiring a standalone representation of the document discourse. It additionally leads to a significant scalability advantage since the encoding of the answer candidate phrases in the document can be pre-computed and indexed offline for efficient retrieval. We experiment with baseline models for the new task, which achieve a reasonable accuracy but significantly underperform unconstrained QA models. We invite the QA research community to engage in Phrase-Indexed Question Answering (PIQA, pika) for closing the gap.')

block extralinks
  link(rel='stylesheet', href='/stylesheets/index.css')
  script(async defer src="https://buttons.github.io/buttons.js")

block extrascripts

mixin piqa_model_display(group, is_test)
  table.table.performanceTable
    tr
      if is_test
        th Rank
      th Model
      th EM
      th F1
    - var human_em = 86.831
    - var human_f1 = 89.452
    - var largest_em = Math.max.apply(null, group.map(function (model) { return model.em; }))
    - var largest_f1 = Math.max.apply(null, group.map(function (model) { return model.f1; }))
      tr.human-row
        td
        td
          | BERT (single model) - Unconstrained
          p.institution Google AI Language
          a(href="https://arxiv.org/abs/1810.04805") (Devlin et al., '18)
        td 85.083
        td 91.835
      tr.human-row
        td
        td
          | Human Performance
          p.institution Stanford University
          a(href="http://arxiv.org/abs/1606.05250") (Rajpurkar & Jia et al. '18)
        td #{human_em}
        td #{human_f1}
    each model in group
      tr
        if is_test
          td 
            p #{model.rank}
            span.date.label.label-default #{moment.unix(model.date).format('MMM DD, YYYY')}
        td(style="word-break:break-word;")
          | #{model.model_name}
          p.institution #{model.institution}
          if model.link
            a.link(href=model.link) #{model.link}
        td
          if model.em == largest_em
            b #{model.em.toPrecision(5)}
          else
            | #{model.em.toPrecision(5)}
        td
          if model.f1 == largest_f1
            b #{model.f1.toPrecision(5)}
          else
            | #{model.f1.toPrecision(5)}

block content
  .cover#contentCover
    .container
      .row
        .col-md-5
          .infoCard
            .infoBody
              .infoHeadline
                h2 What is PIQA?
              p 
                span
                  b P
                  |hrase-
                  b I
                  |ndexed 
                  b Q
                  |uestion 
                  b A
                  |nswering (PIQA) 
                | is a new variant of question answering tasks enforcing complete 
                i independence 
                | of the document encoder from the question encoder. This task addresses a key challenge in machine comprehension by requiring a standalone representation of the document discourse.  We invite the QA research community to engage in Phrase-Indexed Question Answering (PIQA, 
                i pika
                | ) for closing the gap between PIQA baseline models and unconstrained QA models.
              p
              .infoHeadline
                h2 Why PIQA?
              p
                | PIQA brings a significant scalability advantage since the encoding of the answer candidate phrases in the document can be pre-computed and indexed offline for efficient answer retrieval. Unline existing QA models, PIQA does not require the recomputation of document representations for a new question.
              hr
              p
                b  PIQA
                |  uses 
                a(href="https://arxiv.org/abs/1606.05250") SQuAD1.1 
                | for the training and the evaluation. For the details of PIQA and its baseline models, please refer to our paper.
              a.btn.actionBtn(href="https://arxiv.org/abs/1804.07726") PIQA paper (Seo et al. EMNLP 2018)
              p
              .infoHeadline
                h2 Getting Started
              p
                | To ensure the independence of document and question encoders, we provide a slight different procedure of generating prediction files on SQuAD1.1. The basic idea is to use 
                code
                  | spilt.py
                |  to split the original dataset into contexts and questions, and then use 
                code
                  | merge.py
                |  to generate the prediction file. We strongly encourage you to read through the encoding procedure described in 
                a(href="https://github.com/uwnlp/piqa/tree/master/squad") PIQA github page.
              p
                | To evaluate your models, we use the official evaluation script provided by SQuAD1.1, along with a sample prediction file that the script will take as input.
                ul.list-unstyled
                  li
                    a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/", download)
                      | Evaluation Script v1.1
                  li
                    a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/bundles/0x8731effab84f41b7b874a070e40f61e2/", download)
                      | Sample Prediction File (on Dev v1.1)
              p Once you have a built a model that works to your expectations on the dev set, you submit it to get official scores on the dev and a hidden test set. To preserve the integrity of test results, we do not release the test set to the public. Instead, we require you to submit your model so that we can run it on the test set for you. Here's a tutorial walking you through official evaluation of your model:
              a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/worksheets/0x8212d84ca41c4150b555a075b19ccc05/")
                | Submission Tutorial
              p To keep up to date with major changes to the PIQA challenge, please subscribe:
              include includes/mailchimp
              .infoHeadline
                h2 Have Questions?
              p 
                | Ask us questions at 
                a(href="mailto:seominjoon@gmail.com") seominjoon@gmail.com
                |  and 
                a(href="mailto:lee.jnhk@gmail.com") lee.jnhk@gmail.com
                | .
            .infoSubheadline
              include includes/tweet
              include includes/github
        .col-md-7
          .infoCard
            .infoBody
              .infoHeadline
                h2 Leaderboard
              p PIQA tests the ability of a system to produce standalone representation of the document discourse. 
              +piqa_model_display(test, true)
